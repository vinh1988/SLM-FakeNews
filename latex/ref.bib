@inproceedings{singhania20193han,
  title={3HAN: A Deep Neural Network for Fake News Detection},
  author={Singhania, Sneha and Fernandez, Nuria and Choi, Jihie},
  booktitle={International Conference on Neural Information Processing},
  pages={572--581},
  year={2019},
  organization={Springer}
}

@article{wolf2019huggingface,
  title={HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and others},
  journal={arXiv preprint arXiv:1910.03771},
  year={2019}
}

@article{devlin2018bert,
  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@inproceedings{verma2021welfake,
  title={{WELFake}: Word Embedding over Linguistic Features for Fake News Detection},
  author={Verma, Pradip K and Agrawal, P and Amorim, I and Prodan, R},
  booktitle={IEEE Transactions on Computational Social Systems},
  number={4},
  pages={881--893},
  year={2021},
  publisher={IEEE}
}

@article{shu2020fakenewsnet,
  title={{FakeNewsNet}: A Data Repository with News Content, Social Context, and Spatiotemporal Information for Studying Fake News on Social Media},
  author={Shu, Kai and Mahudeswaran, Deepak and Wang, Suhang and Lee, Dongwon and Liu, Huan},
  journal={Big Data},
  volume={8},
  number={3},
  pages={171--188},
  year={2020},
  publisher={Mary Ann Liebert, Inc.}
}

@inproceedings{wang2017liar,
	title={{``Liar, Liar Pants on Fire''}: A New Benchmark Dataset for Fake News Detection},
	author={Wang, William Yang},
	booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
	pages={422--426},
	year={2017},
	publisher={Association for Computational Linguistics}
}

@misc{who2020mythbusters,
	author = {{World Health Organization}},
	title = {Coronavirus disease ({COVID-19}) advice for the public: Mythbusters},
	year = {2020},
	howpublished = {\url{https://www.who.int/emergencies/diseases/novel-coronavirus-2019/advice-for-public/myth-busters}},
	note = {Accessed: 2025-01-01}
}

@misc{wolf2020transformers,
	title={Transformers: State-of-the-Art Natural Language Processing}, 
	author={Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and RÃ©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush},
	year={2020},
	eprint={1910.03771},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations}
}

@article{vosoughi2018spread,
	title={The spread of true and false news online},
	author={Vosoughi, Soroush and Roy, Deb and Aral, Sinan},
	journal={Science},
	volume={359},
	number={6380},
	pages={1146--1151},
	year={2018},
	publisher={American Association for the Advancement of Science}
}

@article{shu2017fake,
	title={Fake News Detection on Social Media: A Data Mining Perspective},
	author={Shu, Kai and Sliva, Amy and Wang, Suhang and Tang, Jiliang and Liu, Huan},
	journal={SIGKDD Explorations Newsletter},
	volume={19},
	number={1},
	pages={22--36},
	year={2017}
}

@article{allcott2017social,
	title={Social media and fake news in the 2016 election},
	author={Allcott, Hunt and Gentzkow, Matthew},
	journal={Journal of Economic Perspectives},
	volume={31},
	number={2},
	pages={211--236},
	year={2017}
}

@article{zarocostas2020fight,
	title={How to fight an infodemic},
	author={Zarocostas, John},
	journal={The Lancet},
	volume={395},
	number={10225},
	pages={676},
	year={2020},
	publisher={Elsevier}
}

@misc{oshikawa2018survey,
	title={A Survey on Natural Language Processing for Fake News Detection}, 
	author={Ray Oshikawa and Jing Qian and William Yang Wang},
	year={2018},
	eprint={1811.00770},
	archivePrefix={arXiv},
	primaryClass={cs.CL},
	note={Also published in LREC 2020}
}

@article{li2023survey,
	title={A Survey on Model Compression for Large Language Models},
	author={Li, Zhuuhan and Xu, Munan and Song, Qingshan and Liu, David and Krishnamoorthi, Raghuraman},
	journal={arXiv preprint arXiv:2308.07633},
	year={2023}
}

@article{yang2022fake,
	title={A survey on automatic fake news detection: The core and the incidental},
	author={Yang, Li and Zhou, Xian and others},
	journal={Neurocomputing},
	year={2022},
	publisher={Elsevier}
}

@inproceedings{tang2022gal,
	title={Understanding and Improving Knowledge Distillation},
	author={Tang, Jiaxi and Wang, Rakesh and others},
	booktitle={arXiv preprint arXiv:2002.03532},
	year={2022},
	note={Contextual approximation for Distillation performance}
}

@article{zhang2023survey,
	title={A Survey on Fake News Detection with Deep Learning},
	author={Zhang, Yftah and Ghorbani, Ali A},
	journal={IEEE Access},
	year={2023},
	publisher={IEEE}
}

@inproceedings{devlin2019bert,
	title={{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
	author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	booktitle={Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT)},
	pages={4171--4186},
	year={2019}
}

@article{liu2019roberta,
	title={{RoBERTa}: A Robustly Optimized {BERT} Pretraining Approach},
	author={Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
	journal={arXiv preprint arXiv:1907.11692},
	year={2019}
}

@misc{openai2023gpt4,
	title={{GPT-4} Technical Report}, 
	author={OpenAI},
	year={2023},
	eprint={2303.08774},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@article{raffel2020t5,
	title={Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
	author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
	journal={Journal of Machine Learning Research},
	volume={21},
	number={140},
	pages={1--67},
	year={2020}
}

@article{hinton2015distilling,
	title={Distilling the Knowledge in a Neural Network},
	author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
	journal={arXiv preprint arXiv:1503.02531},
	year={2015},
	note={NIPS 2015 Deep Learning Workshop}
}

@article{sanh2019distilbert,
	title={{DistilBERT}, a distilled version of {BERT}: smaller, faster, cheaper and lighter},
	author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
	journal={arXiv preprint arXiv:1910.01108},
	year={2019}
}

@inproceedings{wang2020minilm,
	title={{MiniLM}: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers},
	author={Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
	booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
	volume={33},
	pages={5776--5788},
	year={2020}
}

@inproceedings{lan2019albert,
	title={{ALBERT}: A Lite {BERT} for Self-supervised Learning of Language Representations},
	author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2020}
}

@article{sun2020mobilebert,
	title={{MobileBERT}: a Compact Task-Agnostic {BERT} for Resource-Limited Devices},
	author={Sun, Zhiqing and Yu, Hongkun and Song, Xiaodan and Liu, Renjie and Yang, Yiming and Zhou, Denny},
	journal={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL)},
	pages={2158--2170},
	year={2020}
}

@inproceedings{jiao2020tinybert,
	title={{TinyBERT}: Distilling {BERT} for Natural Language Understanding},
	author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
	booktitle={Findings of the Association for Computational Linguistics: EMNLP 2020},
	pages={4163--4174},
	year={2020}
}

@inproceedings{han2016deep,
	title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
	author={Han, Song and Mao, Huizi and Dally, William J},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2016}
}

@inproceedings{hu2021lora,
	title={{LoRA}: Low-Rank Adaptation of Large Language Models},
	author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
	booktitle={International Conference on Learning Representations (ICLR)},
	year={2022}
}

@inproceedings{houlsby2019parameter,
	title={Parameter-Efficient Transfer Learning for {NLP}},
	author={Houlsby, Neil and Giurgiu, Andrei and Jastrzebski, Stanislaw and Morrill, Bruna and Corrado, Andrea and Vassilvitskii, Sergei and others},
	booktitle={International Conference on Machine Learning (ICML)},
	pages={2790--2799},
	year={2019},
	organization={PMLR}
}

@inproceedings{lester2021power,
	title={The Power of Scale for Parameter-Efficient Prompt Tuning},
	author={Lester, Brian and Al-Rfou, Rami and Constant, Noah},
	booktitle={Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
	pages={3045--3059},
	year={2021}
}


@inproceedings{ruchansky2017csi,
	title={{CSI}: A Hybrid Deep Model for Fake News Detection},
	author={Ruchansky, Natali and Seo, Sungyong and Liu, Yan},
	booktitle={Proceedings of the 2017 ACM on Conference on Information and Knowledge Management (CIKM)},
	pages={797--806},
	year={2017}
}

@inproceedings{wang2018eann,
	title={{EANN}: Event Adversarial Neural Networks for Multi-Modal Fake News Detection},
	author={Wang, Yaqing and Ma, Fenglong and Jin, Zhiwei and Yuan, Ye and Xun, Guangxu and Jha, Kishlay and Su, Lu and Gao, Jing},
	booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
	pages={849--857},
	year={2018}
}

@inproceedings{singhania20173han,
	title={{3HAN}: A Deep Neural Network for Fake News Detection},
	author={Singhania, Sneha and Fernandez, Nigel and Rao, Shrisha},
	booktitle={International Conference on Neural Information Processing (ICONIP)},
	pages={572--581},
	year={2017},
	publisher={Springer}
}