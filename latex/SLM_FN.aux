\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{vosoughi2018spread}
\citation{shu2017fake,allcott2017social}
\citation{zarocostas2020fight}
\citation{who2020mythbusters}
\citation{oshikawa2018survey}
\@writefile{toc}{\contentsline {title}{An Approach Based on Fine-tuning Small Language Models for Fake News Detection}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\authcount {3}}
\@writefile{toc}{\contentsline {author}{Khac-Lap Phan\and {Quang-Vinh Pham} \and Quang-Hung Le$^*$\unskip \ \ignorespaces  Address: \textit  {Dept. of Information Technology, Quy Nhon University, Vietnam} \unskip \ \ignorespaces  \textit  {TMA Tech Group, Gia Lai, Vietnam} \unskip \ \ignorespaces  Email: {\tt  lap4654100006@st.qnu.edu.vn}, {\tt  pqvinh@tma.com.vn}, {\tt  lequanghung@qnu.edu.vn}$^*$}{1}{chapter.1}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1.1}\protected@file@percent }
\citation{devlin2019bert,openai2023gpt4}
\citation{hinton2015distilling,sanh2019distilbert}
\citation{li2023survey}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background and Related Work}{2}{section.1.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Task Formulation}{2}{subsection.1.2.1}\protected@file@percent }
\citation{shu2017fake,ruchansky2017csi}
\citation{wang2018eann,singhania20193han}
\citation{yang2022fake,liu2019roberta}
\citation{devlin2019bert,liu2019roberta}
\citation{raffel2020t5,openai2023gpt4}
\citation{hinton2015distilling}
\citation{han2016deep}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Fake News Detection Techniques}{3}{subsection.1.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Small Language Models (SLMs)}{3}{subsection.1.2.3}\protected@file@percent }
\citation{sanh2019distilbert,jiao2020tinybert,wang2020minilm,lan2019albert,sun2020mobilebert}
\citation{li2023survey,tang2022gal}
\citation{yang2022fake,zhang2023survey}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Research Gap and Research Questions}{4}{subsection.1.2.4}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {3}Methodology}{4}{section.1.3}\protected@file@percent }
\newlabel{sec:methodology}{{3}{4}{Methodology}{section.1.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}System Overview}{4}{subsection.1.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces System overview of the proposed SLM-based framework. The pipeline proceeds from unified preprocessing and adaptive encoding (via Full FT or PEFT) to final binary classification.\relax }}{5}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:workflow_diagram}{{1}{5}{System overview of the proposed SLM-based framework. The pipeline proceeds from unified preprocessing and adaptive encoding (via Full FT or PEFT) to final binary classification.\relax }{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Models and Fine-Tuning Strategies}{5}{subsection.1.3.2}\protected@file@percent }
\newlabel{subsec:models_strategies}{{3.2}{5}{Models and Fine-Tuning Strategies}{subsection.1.3.2}{}}
\citation{sanh2019distilbert}
\citation{wang2020minilm}
\citation{lan2019albert}
\citation{hu2021lora}
\citation{houlsby2019parameter}
\@writefile{toc}{\contentsline {paragraph}{Full-Parameter Fine-Tuning.}{6}{section*.3}\protected@file@percent }
\@writefile{toc}{\contentsline {paragraph}{PEFT Variants.}{6}{section*.4}\protected@file@percent }
\citation{lester2021power}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Training Procedure}{7}{subsection.1.3.3}\protected@file@percent }
\newlabel{subsec:training_procedure}{{3.3}{7}{Training Procedure}{subsection.1.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Classification Layer}{8}{subsection.1.3.4}\protected@file@percent }
\newlabel{subsec:classification_layer}{{3.4}{8}{Classification Layer}{subsection.1.3.4}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces General Training Framework for SLM-based Fake News Detection\relax }}{8}{algorithm.1}\protected@file@percent }
\newlabel{alg:general_framework}{{1}{8}{General Training Framework for SLM-based Fake News Detection\relax }{algorithm.1}{}}
\citation{verma2021welfake}
\citation{shu2020fakenewsnet}
\citation{wang2017liar}
\@writefile{toc}{\contentsline {section}{\numberline {4}Experimental Setup}{9}{section.1.4}\protected@file@percent }
\newlabel{sec:experimental_setup}{{4}{9}{Experimental Setup}{section.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Datasets}{9}{subsection.1.4.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Effective dataset statistics post-preprocessing. The \textit  {Effective Samples} column reflects the data actually used for training and evaluation after cleaning and label aggregation.\relax }}{9}{table.caption.5}\protected@file@percent }
\newlabel{tab:dataset_stats}{{1}{9}{Effective dataset statistics post-preprocessing. The \textit {Effective Samples} column reflects the data actually used for training and evaluation after cleaning and label aggregation.\relax }{table.caption.5}{}}
\citation{devlin2018bert}
\citation{liu2019roberta}
\citation{wolf2019huggingface}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Baselines and Comparison Models}{10}{subsection.1.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Evaluation Metrics}{10}{subsection.1.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Implementation Details}{10}{subsection.1.4.4}\protected@file@percent }
\citation{verma2021welfake}
\citation{verma2021welfake}
\citation{verma2021welfake}
\citation{verma2021welfake}
\citation{shu2017fake}
\citation{shu2017fake}
\citation{shu2017fake}
\citation{shu2017fake}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Hyperparameter settings for different training strategies.\relax }}{11}{table.caption.6}\protected@file@percent }
\newlabel{tab:hyperparams}{{2}{11}{Hyperparameter settings for different training strategies.\relax }{table.caption.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{11}{section.1.5}\protected@file@percent }
\newlabel{sec:results_discussion}{{5}{11}{Results}{section.1.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Experimental Results}{11}{subsection.1.5.1}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Results on WELFake. Adapters achieve high accuracy with moderate speed.\relax }}{12}{table.caption.7}\protected@file@percent }
\newlabel{tab:welfake_results}{{3}{12}{Results on WELFake. Adapters achieve high accuracy with moderate speed.\relax }{table.caption.7}{}}
\newlabel{fig:welfake_barchart}{{2a}{13}{Bar chart showing F1 scores of different models on the WELFake dataset~\cite {verma2021welfake}.\relax }{figure.caption.8}{}}
\newlabel{sub@fig:welfake_barchart}{{a}{13}{Bar chart showing F1 scores of different models on the WELFake dataset~\cite {verma2021welfake}.\relax }{figure.caption.8}{}}
\newlabel{fig:welfake_scatter}{{2b}{13}{Scatter plot of inference speed vs. accuracy for models on the WELFake dataset.\relax }{figure.caption.8}{}}
\newlabel{sub@fig:welfake_scatter}{{b}{13}{Scatter plot of inference speed vs. accuracy for models on the WELFake dataset.\relax }{figure.caption.8}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Results on the WELFake dataset\nobreakspace  {}\cite  {verma2021welfake}. (a) F1 scores across model families. (b) Speed-accuracy trade-off visualization.\relax }}{13}{figure.caption.8}\protected@file@percent }
\newlabel{fig:res_welfake}{{2}{13}{Results on the WELFake dataset~\cite {verma2021welfake}. (a) F1 scores across model families. (b) Speed-accuracy trade-off visualization.\relax }{figure.caption.8}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Results on FakeNewsNet. Adapters outperform Prompt Tuning on imbalanced data.\relax }}{14}{table.caption.9}\protected@file@percent }
\newlabel{tab:fakenewsnet_results}{{4}{14}{Results on FakeNewsNet. Adapters outperform Prompt Tuning on imbalanced data.\relax }{table.caption.9}{}}
\newlabel{fig:fakenewsnet_barchart}{{3a}{15}{Bar chart of model performance on the FakeNewsNet dataset~\cite {shu2017fake}.\relax }{figure.caption.10}{}}
\newlabel{sub@fig:fakenewsnet_barchart}{{a}{15}{Bar chart of model performance on the FakeNewsNet dataset~\cite {shu2017fake}.\relax }{figure.caption.10}{}}
\newlabel{fig:fakenewsnet_scatter}{{3b}{15}{Scatter plot of computational efficiency metrics on the FakeNewsNet dataset.\relax }{figure.caption.10}{}}
\newlabel{sub@fig:fakenewsnet_scatter}{{b}{15}{Scatter plot of computational efficiency metrics on the FakeNewsNet dataset.\relax }{figure.caption.10}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Results on the FakeNewsNet dataset\nobreakspace  {}\cite  {shu2017fake}. (a) Performance comparison. (b) Efficiency metrics.\relax }}{15}{figure.caption.10}\protected@file@percent }
\newlabel{fig:res_fakenewsnet}{{3}{15}{Results on the FakeNewsNet dataset~\cite {shu2017fake}. (a) Performance comparison. (b) Efficiency metrics.\relax }{figure.caption.10}{}}
\bibstyle{unsrt}
\bibdata{ref}
\bibcite{vosoughi2018spread}{1}
\bibcite{shu2017fake}{2}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{16}{section.1.6}\protected@file@percent }
\newlabel{sec:conclusion}{{6}{16}{Conclusion}{section.1.6}{}}
\bibcite{allcott2017social}{3}
\bibcite{zarocostas2020fight}{4}
\bibcite{who2020mythbusters}{5}
\bibcite{oshikawa2018survey}{6}
\bibcite{devlin2019bert}{7}
\bibcite{openai2023gpt4}{8}
\bibcite{hinton2015distilling}{9}
\bibcite{sanh2019distilbert}{10}
\bibcite{li2023survey}{11}
\bibcite{ruchansky2017csi}{12}
\bibcite{wang2018eann}{13}
\bibcite{singhania20193han}{14}
\bibcite{yang2022fake}{15}
\bibcite{liu2019roberta}{16}
\bibcite{raffel2020t5}{17}
\bibcite{han2016deep}{18}
\bibcite{jiao2020tinybert}{19}
\bibcite{wang2020minilm}{20}
\bibcite{lan2019albert}{21}
\bibcite{sun2020mobilebert}{22}
\bibcite{tang2022gal}{23}
\bibcite{zhang2023survey}{24}
\bibcite{hu2021lora}{25}
\bibcite{houlsby2019parameter}{26}
\bibcite{lester2021power}{27}
\bibcite{verma2021welfake}{28}
\bibcite{shu2020fakenewsnet}{29}
\bibcite{wang2017liar}{30}
\bibcite{devlin2018bert}{31}
\bibcite{wolf2019huggingface}{32}
\gdef \@abspage@last{18}
